{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Environment preparation"],"metadata":{"id":"UudoBtdA-_Nl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JVP5Ner0-07R","executionInfo":{"status":"ok","timestamp":1674532163903,"user_tz":-60,"elapsed":3986,"user":{"displayName":"FRANCESCO PUGNALONI","userId":"15672983401088559361"}},"outputId":"1a6947fb-620f-4734-a511-37efd40ceef3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: duckdb in /usr/local/lib/python3.8/dist-packages (0.6.1)\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.8/dist-packages (from duckdb) (1.21.6)\n"]}],"source":["pip install duckdb"]},{"cell_type":"code","source":["pip install \"kshingle>=0.10.0,<1\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gEOQ7dh_EXN","executionInfo":{"status":"ok","timestamp":1674532167804,"user_tz":-60,"elapsed":3908,"user":{"displayName":"FRANCESCO PUGNALONI","userId":"15672983401088559361"}},"outputId":"a73befd8-4eb9-45b5-9f7b-11752c807667"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kshingle<1,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (0.10.0)\n","Requirement already satisfied: numpy<2,>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from kshingle<1,>=0.10.0) (1.21.6)\n","Requirement already satisfied: numba>=0.52.0 in /usr/local/lib/python3.8/dist-packages (from kshingle<1,>=0.10.0) (0.56.4)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.52.0->kshingle<1,>=0.10.0) (6.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.52.0->kshingle<1,>=0.10.0) (57.4.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.52.0->kshingle<1,>=0.10.0) (0.39.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.52.0->kshingle<1,>=0.10.0) (3.11.0)\n"]}]},{"cell_type":"code","source":["pip install datasketch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Y6LoA1C_HZJ","executionInfo":{"status":"ok","timestamp":1674532172210,"user_tz":-60,"elapsed":4413,"user":{"displayName":"FRANCESCO PUGNALONI","userId":"15672983401088559361"}},"outputId":"4ee0ede4-8478-4b86-b6e0-32a26ad991d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasketch in /usr/local/lib/python3.8/dist-packages (1.5.8)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from datasketch) (1.21.6)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasketch) (1.7.3)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import duckdb\n","from tabulate import tabulate\n","import kshingle as ks\n","import datasketch\n","import time\n","import random\n","from itertools import combinations\n","import matplotlib.pyplot as plt"],"metadata":{"id":"ZiDwZYkv_I5g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utility functions"],"metadata":{"id":"Mr-jpQaZ_Kbf"}},{"cell_type":"code","source":["def select_all(dbcon, table_name):\n","  query = \"SELECT * FROM \" + table_name\n","  print(dbcon.execute(query).fetchall())"],"metadata":{"id":"VP0XTv3-_J97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_table(tab):\n","  table = tabulate(tab, tab.keys(), tablefmt=\"fancy_grid\")\n","  print(table)"],"metadata":{"id":"avdx7CzE_PSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. database_name: the name of the duckdb database to connect with\n","Output: a database connection\n","Descriprion: create with duckdb or creates it from scratch(if the the file does not exist)\n","'''\n","def open_connection(database_name):\n","  dbcon = duckdb.connect(database=database_name, read_only=False)\n","  return dbcon"],"metadata":{"id":"UDJMyHRP_Q2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Write csv function\n"],"metadata":{"id":"16Sij4t__TSx"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: the connection with the database\n","  2. table_name: the name of the table to create\n","  3. csv_name: the name of the csv file to read\n","Output: none\n","Description: it creates a new table starting from a csv file inside the database\n","'''\n","def write_csv(dbcon, table_name, csv_name):\n","  print('Write csv starting')\n","  query1 = \"DROP TABLE IF EXISTS \" + table_name + \";\"\n","  query2 = \"CREATE TABLE \" + table_name + \" AS SELECT * FROM read_csv_auto('\" + csv_name + \"');\"\n","  dbcon.execute(query1)\n","  dbcon.execute(query2)\n","  print('Write csv ending')"],"metadata":{"id":"MYYNvwB4_SE-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Indexed table creation"],"metadata":{"id":"O9rLlSnn_ZNE"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: the connection with the database\n","  2. table_name: the name of the table to index\n","  3. column_list: the list of columns to analyze \n","Output: none\n","Description: create a new table called __table_name_indexed copying the rows of the old one(removing rows with null value for all the columns to analyze) and adding a column called __table_name_indexes containing an increasing index\n","'''\n","def create_indexed_table(dbcon, table_name, column_list):\n","\n","  query1 = \"DROP TABLE IF EXISTS \" + \"__\" + table_name + \"_indexed\" + \";\"\n","  columns = ''\n","  for i in range(len(column_list)):\n","    if i == 0:\n","      columns = columns + column_list[i] + \" IS NOT NULL \"\n","    else:\n","      columns = columns + \"OR \" + column_list[i] + \" IS NOT NULL \"\n","\n","  query2 = \"CREATE TABLE \" + \"__\" + table_name + \"_indexed\" + \" AS SELECT *, ROW_NUMBER() OVER () AS \" + \"__\" + table_name + \"_indexes\" + \" FROM \" + table_name + \" WHERE \" + columns\n","  dbcon.execute(query1)\n","  dbcon.execute(query2)"],"metadata":{"id":"k53-VqlJ_brL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text view creation"],"metadata":{"id":"tw78bhlr_dYN"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: the connection with the database\n","  2. table_name: the name of the original table\n","  3. indexed_table_name: the name of the table from which the view will be created\n","  4. column_list: list of columns to analyze\n","Output: none\n","Description: create a view on the indexed table to show the index and a new column called \"to_analyze\" obtained concatenating all the columns to analyze\n","'''\n","def create_text_view(dbcon, table_name, indexed_table_name, column_list):\n","  query1 = \"DROP VIEW IF EXISTS \" + \"__\" + table_name + \"_text_view\"+ \";\"\n","  columns = ''\n","  for i in range(len(column_list)):\n","    if i == 0:\n","      columns = columns + column_list[i]\n","    else:\n","      columns = columns + ', ' +column_list[i]\n","  query2 = \"CREATE VIEW \" + \"__\" + table_name + \"_text_view AS SELECT __\" + table_name + \"_indexes, \" + \"CONCAT(\"+ columns+\") AS to_analyze\" + \" FROM \"+\"__\" + table_name + \"_indexed\"+ \";\"\n","  dbcon.execute(query1)\n","  dbcon.execute(query2)"],"metadata":{"id":"m5F1ojv8_f9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fetch into array"],"metadata":{"id":"GHhBFrMo_ig0"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: the connection with the database\n","  2. table_name: the name of the table/view to fetch into the array\n","  3. rows_limit: max number of rows to select\n","Output: a numpy array representation of the view\n","Description: the function read the view in the database and fetch it into an array\n","'''\n","def fetch_view_array(dbcon, table_name, rows_limit):\n","  if rows_limit == False:\n","    query = \"SELECT * FROM \" + table_name\n","  else:\n","    query = \"SELECT * FROM \" + table_name + \" LIMIT \" + str(rows_limit)\n","  out = dbcon.execute(query).fetchnumpy()\n","  return out"],"metadata":{"id":"Z2Y_bEnt_k50"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Shingling"],"metadata":{"id":"NePd3-M2_mWZ"}},{"cell_type":"code","source":["def count_shingle_occ(documents, k_shingle_length):\n","  shingle_occ_dict = {}\n","  for i in range(len(documents)):\n","    tmp = []\n","    for j in ks.shingleset_range(documents[i], k_shingle_length, k_shingle_length):\n","      if j.isascii() and j.isalpha():\n","        try:\n","          shingle_occ_dict[j] += 1\n","        except:\n","          shingle_occ_dict[j] = 1\n","  return shingle_occ_dict"],"metadata":{"id":"IMLcrydA_tkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. base_view_array: a dictionary containing 2 numpy array, one composed by the indexes of the documents and the other one by the concatenated columns to analyze\n","  2. k_shingle_length: the chosen length for the shingles\n","  3. column_name: key to access the concatenated columns array\n","  4. index_name: key to access the indexes array\n","  5. drop_shingles_threshold: a number between 0 and 1, it can be used to filter the created shingles dropping the most frequent ones, specifically this threshold tells the function to drop all the shingles that occurs in more than the drop_shingles_threshold% of the documents\n","Output: a new dictionary containing the shingle matrix(a list of lists), the array of indexes and the number of shingles created over the entire dataset\n","'''\n","\n","def create_k_shingle(base_view_array, k_shingle_length, column_name, index_name, drop_shingles_threshold):\n","  print(\"Shingling phase starts______________\")\n","  start_time = time.time_ns() / 1000000\n","\n","  documents = base_view_array[column_name]\n","\n","  #compute shingles occ\n","  shingles_occ = count_shingle_occ(documents, k_shingle_length)\n","\n","  #shingle matrix creation\n","  n_doc=documents.shape[0]\n","  shingle_indexes = {}\n","  shingle_matrix = []\n","  new_index = 1\n","  for i in range(len(documents)):\n","    tmp = []\n","    for j in ks.shingleset_range(documents[i], k_shingle_length, k_shingle_length):\n","      #print(j)  #debug\n","      if j.isascii() and j.isalpha():\n","        if shingles_occ[j]/n_doc < drop_shingles_threshold:\n","          if j.lower() in shingle_indexes:\n","            tmp.append(shingle_indexes[j.lower()])\n","          else:\n","            shingle_indexes[j.lower()] = new_index\n","            tmp.append(new_index)\n","            new_index += 1\n","    shingle_matrix.append(set(tmp))\n","  out = {index_name:base_view_array[index_name], 'shingle_matrix':shingle_matrix, 'num_shingles':new_index-1}\n","  #print(shingles_occ) #debug\n","  #print(shingle_indexes)  #debug\n","  print('Shingles created: ' + str(new_index-1))\n","  end_time = time.time_ns() / 1000000\n","  print('t_exec: '+str(end_time-start_time)+'ms')\n","  print(\"Shingling phase ends______________\")\n","  return out"],"metadata":{"id":"3sGQFEZJ_og0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Shingling not dropping blanks"],"metadata":{"id":"c_iOKIG-_qaR"}},{"cell_type":"code","source":["def count_shingle_occ_with_blanks(documents, k_shingle_length):\n","  shingle_occ_dict = {}\n","  for i in range(len(documents)):\n","    tmp = []\n","    for j in ks.shingleset_range(documents[i].replace(\" \",\"\"), k_shingle_length, k_shingle_length):\n","      if j.isascii() and j.isalpha():\n","        try:\n","          shingle_occ_dict[j] += 1\n","        except:\n","          shingle_occ_dict[j] = 1\n","  return shingle_occ_dict"],"metadata":{"id":"UIkAImSUKbR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Note: it is a variant of the previous one, here we don't drop spaces\n","'''\n","def create_k_shingle_with_blanks(base_view_array, k_shingle_length, column_name, index_name, drop_shingles_threshold):\n","  print(\"Shingling phase starts______________\")\n","  start_time = time.time_ns() / 1000000\n","\n","  documents = base_view_array[column_name]\n","  #compute shingles occ\n","  shingles_occ = count_shingle_occ_with_blanks(documents, k_shingle_length)\n","\n","  #shingle  matrix creation\n","  n_doc=documents.shape[0]\n","  shingle_indexes = {}\n","  shingle_matrix = []\n","  new_index = 1\n","  for i in range(len(documents)):\n","    tmp = []\n","    for j in ks.shingleset_range(documents[i].replace(\" \",\"\"), k_shingle_length, k_shingle_length):\n","      if j.isascii() and j.isalpha():\n","        if shingles_occ[j]/n_doc < drop_shingles_threshold:\n","          if j.lower() in shingle_indexes:\n","            tmp.append(shingle_indexes[j.lower()])\n","          else:\n","            shingle_indexes[j.lower()] = new_index\n","            tmp.append(new_index)\n","            new_index += 1\n","    shingle_matrix.append(set(tmp))\n","  out = {index_name:base_view_array[index_name], 'shingle_matrix':shingle_matrix, 'num_shingles':new_index-1}\n","\n","  #print(shingles_occ) #debug\n","  #print(shingle_indexes)  #debug\n","\n","  print('Shingles created: ' + str(new_index))\n","  end_time = time.time_ns() / 1000000\n","  print('t_exec: '+str(end_time-start_time)+'ms')\n","  print(\"Shingling phase ends______________\")\n","  return out"],"metadata":{"id":"vZMZ8T6m_qMB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Minhashing"],"metadata":{"id":"-rD3oPiX_2OL"}},{"cell_type":"code","source":["#Note: this function is not collision proof, finding a better one would be good\n","def create_hash_function(p, num_shingles):\n","  a = random.randint(1, p - 1)  \n","  b = random.randint(0, p - 1)  \n","  return lambda x : (((a * x + b) % p) % num_shingles)  # hash function\n","\n","def create_hash_family(num_shingles, num_func=128, large_prime=1000000000039):\n","  out = []\n","  for i in range(num_func):\n","    out.append(create_hash_function(large_prime, num_shingles))\n","  return out"],"metadata":{"id":"rhhtaCXS_4fL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. shingle_matrix: the output of the create_k_shingle function\n","  2. minhashing_threshold: it allow to stop minhashing before it tries all the permutations\n","Output: the shingle matrix privided by input after the needed modifications\n","Description: this function add an entry to the shingle matrix that contains a numpy minhashed matrix, this matrix has one row for each document and one column for each hash to compute\n","'''\n","def minhash(shingle_matrix, minhashing_threshold=0.2):\n","  print(\"Minhashing phase starts______________________________________\")\n","  start_time = time.time_ns() / 1000000\n","  shingled_documents = shingle_matrix['shingle_matrix']\n","  num_shingles = shingle_matrix['num_shingles']\n","  \n","  #hash functions creation\n","  hash_funcs = create_hash_family(num_shingles)\n","\n","  #minhashed matrix initizlization\n","  minhashed = np.zeros(128*len(shingled_documents)).reshape(len(shingled_documents),128)\n","\n","  for i in range(len(hash_funcs)):\n","    if i%10 == 0:\n","      print('Hash number: ' + str(i)+'/128') #debug\n","    start_time_epoche = time.time_ns() / 1000000\n","\n","    f = hash_funcs[i]\n","    docs = list(range(len(shingled_documents)))\n","    for j in range(0,num_shingles):\n","      #if (j%1000) == 0:  #debug\n","        #print('Index number: '+str(j)+'/'+str(num_shingles)) #debug\n","        #print('Documents still to analyze: '+str(len(docs))+'/'+str(len(shingled_documents))+' = '+str(len(docs)/len(shingled_documents))) #debug\n","      index = f(j)+1\n","      for d in list(docs):\n","        if index in shingled_documents[d]:\n","          minhashed[d,i] = j+1\n","          docs.remove(d)\n","      if docs == [] or (len(docs)/len(shingled_documents)) < minhashing_threshold:\n","        break\n","    end_time_epoche = time.time_ns() / 1000000\n","    if i % 10 == 0:\n","      print('t_exec last epoche: '+str(end_time_epoche-start_time_epoche)+'ms')\n","  \n","  #print(minhashed)  #debug\n","\n","  shingle_matrix['minhashed_matrix'] = minhashed\n","  end_time = time.time_ns() / 1000000\n","  print('t_exec: '+str(end_time-start_time)+'ms')\n","  print(\"Minhashing phase ends______________________________________\")\n","  return shingle_matrix"],"metadata":{"id":"cLhAKqOH_7gc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Write minhash function"],"metadata":{"id":"VGCRxB3e_-aF"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: connection to duckdb\n","  2. minhash_array: the output of the minhash function\n","  3. index_name: key to access the indexes array\n","  4. table_name: name of the table to analyze\n","Output: none\n","Description: it creates a table containing a row for every document, the first column is the index of the document and the other ones are the minhash\n","'''\n","def write_minhash(dbcon, minhash_array, index_name, table_name):\n","  print('Writing minhashes in the database____________')\n","  minhash = minhash_array['minhashed_matrix']\n","  indexes = minhash_array[index_name]\n","  tmp = {index_name:indexes}\n","\n","  for i in range(128):\n","    tmp[i]=minhash_array['minhashed_matrix'][:,i]\n","  tmp = pd.DataFrame.from_dict(tmp)\n","\n","  query1 = \"DROP TABLE IF EXISTS \" + \"__\" + table_name + \"_minhash\" + \";\"\n","  query2 = \"CREATE TABLE \" + \"__\" + table_name + \"_minhash\" + \" AS SELECT * FROM tmp\"\n","  dbcon.execute(query1)\n","  dbcon.execute(query2)\n","  print('Minhashes written in the database___________')"],"metadata":{"id":"y17vWR_yAAxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Read minhash function"],"metadata":{"id":"fIW3Z45yACSd"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: connection to duckdb\n","  3. index_name: key to access the indexes array\n","  4. table_name: name of the table to analyze\n","Output: a dictionary containing 2 entries, the first one is a 1-d numpy array containing the indexes, the second one is a 2-d numpy array containing the minhashes of the documents\n","'''\n","def read_minhash(dbcon, table_name, index_name):\n","  print('Reading minhashing from the database__________________')\n","  raw_minhash_array = fetch_view_array(dbcon, \"__\" + table_name + \"_minhash\", False)\n","  out = {index_name:raw_minhash_array[index_name]}\n","  column_list = []\n","  \n","  for i in range(128):\n","    column_list.append(raw_minhash_array[str(i)].reshape(1,len(raw_minhash_array['0'])))\n","\n","  tmp = np.concatenate(column_list,axis=0).T\n","  out['minhashed_matrix'] = tmp\n","  print('Minhashes ridden from the database__________________')\n","  return out"],"metadata":{"id":"z_Mb6FMyAEaj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LSH python"],"metadata":{"id":"2Gi0Q97VAH34"}},{"cell_type":"code","source":["def compute_jaccard(a,b):\n","  return (a == b).sum() / a.shape[0]\n","\n","'''\n","Parameters:\n","  1. minhashed_matrix: the output of read minhash funciton\n","  2. b: number of bands\n","  3. threshold: similarity threshold for documents\n","  4. indexes_name: key to access the indexes array\n","Output: a set containing all the found similar pairs\n","'''\n","def compute_LSH_python(minhashed_matrix, b, threshold, indexes_name):\n","  print('Python LSH computation phase starts______________________________________________')\n","  start_time = time.time_ns() / 1000000\n","  #bands creation\n","  minhashes = minhashed_matrix['minhashed_matrix']\n","  indexes = minhashed_matrix[indexes_name]\n","  num_band_values = minhashes.shape[1] / b\n","  \n","  #buckets creation\n","  # O(n_bande)\n","  band_list = []\n","  for i in range(b):\n","    band_list.append({})\n","\n","  #ranges creation\n","  # O(n_bande)\n","  ranges = []\n","  tmp = 0\n","  while tmp < 128:\n","    a = tmp\n","    tmp += num_band_values\n","    b = tmp\n","    c = (int(a),int(b))\n","    ranges.append(c)\n","\n","  #buckets filling\n","  # O(n_bande*n_doc)\n","  print('Filling buckets')\n","  for i in range(len(ranges)):\n","    for j in range(minhashes.shape[0]):\n","      bucket_key = minhashes[j, ranges[i][0]:ranges[i][1] ].sum()\n","      if bucket_key != 0:\n","        try:\n","          band_list[i][bucket_key].append(j)\n","        except:\n","          band_list[i][bucket_key] = [j]\n","  print('Buckets filled')\n","\n","  print('Candidates creation starts')\n","  candidates = []\n","  for ba in band_list:\n","    for bu in ba.values():\n","      tmp = list(combinations(bu,2))\n","      candidates += tmp\n","  candidates = set(candidates)\n","  print('Candidates creation ends')\n","\n","  print('False positive eliminations starts')\n","  start_time_pairs = time.time_ns() / 1000000\n","  pairs = []\n","  for c in candidates:\n","    if compute_jaccard(minhashes[c[0]], minhashes[c[1]]) >= threshold:\n","      new_pair = (indexes[c[0]], indexes[c[1]])\n","      pairs.append(new_pair)\n","      #print(str(c[0])+':'+str(c[1])+'='+str(compute_jaccard(minhashes[c[0]], minhashes[c[1]]))) #debug\n","  end_time_pairs = time.time_ns() / 1000000\n","  print('t_exec false elim: '+str(end_time_pairs-start_time_pairs)+'ms')\n","  print('False positive eliminations ends')\n","  \n","  pairs = set(pairs)\n","  end_time = time.time_ns() / 1000000\n","  print('____________________t_exec lsh python: '+str(end_time-start_time)+'ms')\n","  print('Python LSH computation phase ends________________________________________________')\n","  return pairs"],"metadata":{"id":"wjXdVxh0AJaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LSH SQL"],"metadata":{"id":"WeBX5GqAAPAN"}},{"cell_type":"code","source":["def get_column_sum(a,b, t_name):\n","  out = \"\"\n","  for i in range(a,b):\n","    if i != a:\n","      out += '+'\n","    out = out + t_name + '.' +'\"'+str(i)+'\"'\n","  return out\n","\n","def get_query_band(a,b,minhashes_table_name,indexes_name):\n","  l_sum = get_column_sum(a,b,'m1')\n","  r_sum = get_column_sum(a,b,'m2')\n","  query = \"select m1.\"+indexes_name+' as l_doc_id, m2.'+indexes_name+' as r_doc_id from '+minhashes_table_name+' m1, '+minhashes_table_name+' m2 '\n","  query = query + 'where m1.'+indexes_name+' < m2.'+indexes_name+' and ' + l_sum + ' = '+r_sum\n","  return query\n","\n","def get_intersect_string(num_hash):\n","  out = 'case when p1.\"0\"=p2.\"0\" then 1 else 0 end '\n","  for i in range(1,num_hash):\n","    out = out + ' +case when p1.\"'+str(i)+'\"=p2.\"'+str(i)+'\" then 1 else 0 end '\n","  return out\n","\n","def get_union_string(num_hash, intersect_string):\n","  out = str(128)\n","  return out\n","\n","def get_argument_string(intersect_string, union_string):\n","  out = \"cast((\"+intersect_string+\") as DOUBLE) / cast((\"+union_string+\") as DOUBLE)\"\n","  return out\n","'''\n","Parameters:\n","  1. dbcon: connection to duckdb\n","  2. table_name: name of the table to analyze\n","  3. minhashes_table_name: name of the minhashes table saved inside duckdb\n","  4. bands: number of bands\n","  5. threshold: similarity threshold for documents\n","  6. indexes_name:  key to access the indexes array\n","  7. num_hash: number of hashes computed from minhash\n","Output: none\n","Description: it computes the lsh without reading data from the db\n","'''\n","def compute_LSH_SQL(dbcon, table_name, minhashes_table_name, bands, threshold, indexes_name, num_hash=128):\n","  print('SQL LSH computation phase starts______________________________________________')\n","  start_time = time.time_ns() / 1000000\n","  # Intervals list creation\n","  num_band_values = num_hash / bands\n","  ranges = []\n","  tmp = 0\n","  while tmp < num_hash:\n","    a = tmp\n","    tmp += num_band_values\n","    b = tmp\n","    c = (int(a),int(b))\n","    ranges.append(c)\n","  \n","  # Index pairs table creation\n","  dup_pairs_table_name = '__' + table_name + '_pairs_index_dup'\n","  pairs_table_name = '__' + table_name + '_pairs_index'\n","  query1 = \"DROP TABLE IF EXISTS \" + dup_pairs_table_name + \";\"\n","  query2 = \"CREATE TABLE \" + dup_pairs_table_name + \"(l_doc_id INT, r_doc_id INT);\"\n","  #query2 = \"CREATE TABLE \" + pairs_table_name + \"(l_doc_id INT, r_doc_id INT, PRIMARY KEY(l_doc_id, r_doc_id));\"\n","  dbcon.execute(query1)\n","  dbcon.execute(query2)\n","\n","  # Index pairs table filling\n","  for ba in ranges:\n","    a = ba[0]\n","    b = ba[1]\n","    query_index_band = get_query_band(a,b, minhashes_table_name,indexes_name)\n","    #print('___________________')\n","    #print(dbcon.execute(query_index_band).fetchall())\n","    query_insert = \"insert into \"+dup_pairs_table_name+\" \"+query_index_band\n","    dbcon.execute(query_insert)\n","  \n","  # Duplicates elimination\n","  query3 = \"DROP TABLE IF EXISTS \" + pairs_table_name + \";\"\n","  query4 = \"CREATE TABLE \" + pairs_table_name + \" as \" + \"select distinct * from \"+dup_pairs_table_name\n","  dbcon.execute(query3)\n","  dbcon.execute(query4)\n","  dbcon.execute(query1)\n","\n","  #print('Candidate pairs list:')  #debug\n","  #print(dbcon.execute(\"select * from \"+pairs_table_name).fetchall())  #debug\n","\n","  # False positives elimination\n","  intersect_string = get_intersect_string(num_hash)\n","  union_string = get_union_string(num_hash, intersect_string)\n","  argument_string = get_argument_string(intersect_string, union_string)\n","  select_string = \"select \"+argument_string+\" from \"+minhashes_table_name+\" p1, \"+ minhashes_table_name+\" p2 where p1.\"+indexes_name+\"=l_doc_id and p2.\"+indexes_name+\"=r_doc_id\"\n","  delete_string = \"delete from \"+pairs_table_name+\" where \"+str(threshold)+\">(\"+select_string+\")\"\n","  dbcon.execute(delete_string)\n","  #print(dbcon.execute(\"select * from \"+pairs_table_name).fetchall())\n","  \n","  end_time = time.time_ns() / 1000000\n","  print('____________________t_exec lsh SQL: '+str(end_time-start_time)+'ms')\n","  print('SQL LSH computation phase ends________________________________________________')\n","  return"],"metadata":{"id":"a0L_IO_KAQkc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pairs persist function for python lsh"],"metadata":{"id":"qLNPn8hKAV_3"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: connection to duckdb\n","  2. table_name: name of the table to analyze\n","  3. column_name: name of the column of concatenated columns\n","  4. pairs: list of similar pairs\n","Output: True if the write operation is successful\n","'''\n","def persist(dbcon, table_name, column_name, pairs):\n","  tmp = {}\n","  for i in pairs:\n","    try:\n","      tmp['l_doc_id'].append(i[0])\n","      tmp['r_doc_id'].append(i[1])\n","    except:\n","      tmp['l_doc_id'] = [i[0]]\n","      tmp['r_doc_id'] = [i[1]]\n","  tmp = pd.DataFrame(tmp)\n","  if tmp.shape[0] < 1:\n","    print(\"Pairs not found, nothing to persist\")\n","    return False\n","  query1 = \"DROP TABLE IF EXISTS \" + \"__\" + table_name + \"_pairs_index\" + \";\"\n","  query2 = \"CREATE TABLE \" + \"__\" + table_name + \"_pairs_index\" + \" AS SELECT * FROM tmp\"\n","  dbcon.execute(query1)\n","  dbcon.execute(query2)\n","\n","  \n","  query1 = \"DROP VIEW IF EXISTS \" + \"__\" + table_name + \"_pairs\" + \";\"\n","  query2 = \"CREATE VIEW \" + \"__\" + table_name + \"_pairs\" + \" AS \"+\"SELECT l_doc_id, tl.\"+column_name+\" as l_\"+column_name+\", r_doc_id, tr.\" + column_name + \" as r_\"+column_name+\" FROM \"+\"__\" + table_name + \"_pairs_index\"+\" AS t JOIN \" + \"__\"+table_name+\"_text_view\" \" as tl ON tl.__\" + table_name + \"_indexes = l_doc_id JOIN \"+ \"__\"+table_name+\"_text_view\" + \" as tr ON tr.__\" + table_name + \"_indexes = r_doc_id\"\n","  dbcon.execute(query1)\n","  dbcon.execute(query2)\n","  return True"],"metadata":{"id":"UQMiVFzDAZUS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Show output"],"metadata":{"id":"64e36j2gAbTG"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: connection to duckdb\n","  2. table_name: name of the table to analyze\n","  3. pairs: list of similar pairs found\n","  4. show_similar_contents: if it is True all the similar pairs are displayed, otherwise only the number of found pairs appears\n","Output: none\n","'''\n","def show_output_python(dbcon, table_name, pairs, show_similar_contents):\n","  print(\"_______________Output computed using python\")\n","  print(\"Number of pairs found: \"+str(len(pairs)))\n","  if show_similar_contents:\n","    try:\n","      tmp = dbcon.query(\"select * from __\" + table_name + \"_pairs\").to_df()\n","      print(\"The similar pairs found are the following: \")\n","      print(tmp)\n","    except:\n","      print(\"Table not found, can't display output\")"],"metadata":{"id":"iqne11pPAcq6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. dbcon: connection to duckdb\n","  2. pairs_table_name: name of the table containing the pairs\n","  3. indexes_table_name: name of the indexed table\n","  4. index_column_name: name of the column containing the indexes\n","  5. column_name: name of the analyzed column\n","  6. show_similar_contents: if it is True all the similar pairs are displayed, otherwise only the number of found pairs appears\n","Output: none\n","'''\n","def show_output_SQL(dbcon, pairs_table_name, indexed_table_name,index_column_name,column_name,show_similar_contents):\n","  print(\"_______________Output computed using SQL\")\n","  query = \"select l_doc_id, tl.\"+column_name+\", r_doc_id, tr.\"+column_name+\" from \"+pairs_table_name+\" join \"+indexed_table_name+\" tl on tl.\"+index_column_name+\"=l_doc_id join \"+indexed_table_name+\" tr on tr.\"+index_column_name+\"=r_doc_id\"\n","  tmp = dbcon.query(query).to_df()\n","  if tmp.shape[0] == 0:\n","    print(\"No similar pairs found\")\n","    return\n","  print(\"Number of similar pairs found: \"+str(tmp.shape[0]))\n","  if show_similar_contents == True:\n","    print(\"The similar pairs found are the following: \")\n","    print(tmp)"],"metadata":{"id":"-HmYuzcdAeWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main function"],"metadata":{"id":"TushZ7RHAf1F"}},{"cell_type":"code","source":["'''\n","Parameters:\n","  1. database_name: the name of the .duckdb file\n","  2. table_name: name of the table to analyze\n","  3. column_list: list of the columns to analyze\n","  4. csv_name: optional, if not false it contains the name of a csv file containing the table to insert in duckdb\n","  5. drop_blanks: if True it drop all the shingles containing blanks, otherwise it doesn't\n","  6. lsh_python: if true the function use the python version of the lsh\n","  7. lsh_SQL: if true the function use the SQL version of the lsh\n","  8. compute_minhash: if True the minhash is computed, otherwise we assume to already have a table containing the minhashes inside the db\n","  9. bands: number of bands for lsh\n","  10. show_similar_contents: if true the ouput of lsh is verbose\n","  11. rows_limit: limits the number of rows analyzed\n","  12. k_shingle_length: lenth of the shingles created\n","  13. threshold: threshold for lsh\n","  14. minhashing_threshold: threshold to stop minhashing before it computes all the permutations(lower is stricter)\n","  15. drop_shingles_threshold: a number between greater than 0, it can be used to filter the created shingles dropping the most frequent ones, specifically this threshold tells the function to drop all the shingles that occurs in more than the drop_shingles_threshold% of the documents, obviously passing a number greater than 1 ther won't be drops\n","Output: a dictionary containing the execution times of the main steps of the program\n","'''\n","def lsh(database_name, table_name, column_list, csv_name=False, drop_blanks=True,lsh_python=False,lsh_SQL=True,compute_minhash=True, bands=16, show_similar_contents=True, rows_limit=False, k_shingle_length=5, drop_shingles_threshold=1.1,threshold=0.7,minhashing_threshold=0.2):\n","  #create connection\n","  t_exec = {}\n","  dbcon = open_connection(database_name)\n","  column_name = 'to_analyze'\n","  if compute_minhash:\n","    #write csv in  duck_db\n","    if csv_name != False:\n","      start_time_csv = time.time_ns() / 1000000\n","      write_csv(dbcon, table_name, csv_name)\n","      end_time_csv = time.time_ns() / 1000000\n","      t_exec['csv']=end_time_csv-start_time_csv\n","    \n","    #Create indexed table\n","    create_indexed_table(dbcon, table_name, column_list)\n","\n","    #Create table view\n","    create_text_view(dbcon, table_name, \"__\" + table_name + \"_indexed\", column_list)\n","\n","    #Load numpy array\n","    base_view_array = fetch_view_array(dbcon, \"__\" + table_name + \"_text_view\", rows_limit)\n","\n","    #create k-shingle\n","    start_time_shingle = time.time_ns() / 1000000\n","    if drop_blanks == False:\n","      kshingle_array = create_k_shingle(base_view_array, k_shingle_length, column_name, \"__\" + table_name + \"_indexes\", drop_shingles_threshold)\n","    else:\n","      kshingle_array = create_k_shingle_with_blanks(base_view_array, k_shingle_length, column_name, \"__\" + table_name + \"_indexes\", drop_shingles_threshold)\n","    end_time_shingle = time.time_ns() / 1000000\n","    t_exec['shingle']=end_time_shingle-start_time_shingle\n","\n","    #minhashing\n","    start_time_minhashing = time.time_ns() / 1000000\n","    minhash_array = minhash(kshingle_array,minhashing_threshold=minhashing_threshold)\n","    end_time_minhashing = time.time_ns() / 1000000\n","    t_exec['minhashing']=end_time_minhashing-start_time_minhashing\n","\n","    #write minhashed in duck db\n","    write_minhash(dbcon, minhash_array, \"__\" + table_name + \"_indexes\", table_name)\n"," \n","  #LSH with banding\n","  start_time_lsh_python = time.time_ns() / 1000000\n","  if lsh_python==True:\n","    minhash_array = read_minhash(dbcon, table_name, \"__\" + table_name + \"_indexes\")\n","    pairs = compute_LSH_python(minhash_array, bands, threshold, \"__\" + table_name + \"_indexes\")\n","    isPersisted = persist(dbcon, table_name, column_name, pairs)\n","    show_output_python(dbcon, table_name, pairs, show_similar_contents)\n","  end_time_lsh_python = time.time_ns() / 1000000\n","  t_exec['lsh_python']=end_time_lsh_python-start_time_lsh_python\n","\n","  start_time_lsh_sql = time.time_ns() / 1000000\n","  if lsh_SQL==True:\n","    compute_LSH_SQL(dbcon, table_name, \"__\" + table_name + \"_minhash\", bands, threshold, \"__\" + table_name + \"_indexes\")\n","    show_output_SQL(dbcon, \"__\" + table_name + \"_pairs_index\",  \"__\" + table_name + \"_text_view\", \"__\" + table_name + \"_indexes\",column_name,show_similar_contents)\n","  end_time_lsh_sql= time.time_ns() / 1000000\n","  t_exec['lsh_sql']=end_time_lsh_sql-start_time_lsh_python\n","\n","  return t_exec\n"],"metadata":{"id":"sPTUB96EAg_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"tCy1dOX7Ajiy"}},{"cell_type":"markdown","source":["DuckDB large tables creation"],"metadata":{"id":"KpzBaOj2UNzC"}},{"cell_type":"code","source":["# table with shingle_length=5 and 10000 rows creation\n","#lsh('test.duckdb', 'Blurbs_table', column_list=['Title','Blurb'],lsh_python=True,drop_blanks=True,lsh_SQL=True,csv_name='/content/drive/MyDrive/Colab Notebooks/ProgettoBDMG/csv/books_with_blurbs.csv', minhashing_threshold=0.2, show_similar_contents=True,compute_minhash=True,bands = 16,rows_limit=10000, threshold=0.8, k_shingle_length=5, drop_shingles_threshold=1.1)"],"metadata":{"id":"dWu3rmN7u6T7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tabel creation with shingle of length 5 and max number of rows (almost 50k)\n","#lsh('test.duckdb', 'Blurbs_table_max_length_ks5', column_list=['Title','Blurb'],lsh_python=True,drop_blanks=True,lsh_SQL=True,csv_name='/content/drive/MyDrive/Colab Notebooks/ProgettoBDMG/csv/books_with_blurbs.csv', minhashing_threshold=0.2, show_similar_contents=True,compute_minhash=True,bands = 16,rows_limit=False, threshold=0.8, k_shingle_length=5, drop_shingles_threshold=0.4)"],"metadata":{"id":"ae89Vf952z8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Esecution times computation"],"metadata":{"id":"xK2zmv6zUQqL"}},{"cell_type":"code","source":["# 10k rows\n","out = lsh('/content/drive/MyDrive/Colab Notebooks/ProgettoBDMG/duckdb_versions/BlurbsK5_10k_50k/test.duckdb', 'Blurbs_table', column_list=['Title','Blurb'],lsh_python=True,drop_blanks=True,lsh_SQL=True, minhashing_threshold=0.2, show_similar_contents=True,compute_minhash=False,bands = 16,rows_limit=10000, threshold=0.7, k_shingle_length=5, drop_shingles_threshold=1.1)\n","print(out)"],"metadata":{"id":"0i7AI2jKUIHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lsh('/content/drive/MyDrive/Colab Notebooks/ProgettoBDMG/duckdb_versions/BlurbsK5_10k_50k/test.duckdb', 'Blurbs_table_max_length_ks5', column_list=['Title','Blurb'],lsh_python=True,drop_blanks=True,lsh_SQL=True, minhashing_threshold=0.2, show_similar_contents=True,compute_minhash=False,bands = 16,rows_limit=10000, threshold=0.7, k_shingle_length=5, drop_shingles_threshold=1.1)"],"metadata":{"id":"tuu0eFrBU41b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l = [100, 500, 1000, 5000]\n","texecs = []\n","for i in l:\n","  texecs.append(lsh('test.duckdb', 'TestTable', column_list=['title','Blurb'],lsh_python=True,drop_blanks=True,lsh_SQL=True,csv_name='/content/drive/MyDrive/Colab Notebooks/ProgettoBDMG/csv/books_with_blurbs.csv', minhashing_threshold=0.2, show_similar_contents=True,compute_minhash=True,bands = 16,rows_limit=i, threshold=0.7, k_shingle_length=5, drop_shingles_threshold=1.1))"],"metadata":{"id":"Batn0O0B8zWk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["python=[]\n","sql=[]\n","for i in texecs:\n","  python.append(i['lsh_python'])\n","  sql.append(i['lsh_sql'])\n","python.append(2011.009033203125)\n","python.append(31106)\n","sql.append(11961.82568359375)\n","sql.append(336867)\n","labels=['100','500','1000','5000','10000','57000']"],"metadata":{"id":"r8HZ2srECB_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(python)\n","print(sql)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7E6GybeFRet","executionInfo":{"status":"ok","timestamp":1674535409140,"user_tz":-60,"elapsed":234,"user":{"displayName":"FRANCESCO PUGNALONI","userId":"15672983401088559361"}},"outputId":"b349c62a-ef16-44f7-b27a-7bcef9692b6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[99.9638671875, 141.32275390625, 199.98876953125, 945.99755859375, 2011.009033203125, 31106]\n","[422.06494140625, 530.91357421875, 701.226806640625, 3748.53662109375, 11961.82568359375, 336867]\n"]}]},{"cell_type":"markdown","source":["# References"],"metadata":{"id":"ctQq-GGtsdlh"}},{"cell_type":"markdown","source":["Books with blurb dataset: https://www.kaggle.com/datasets/jdobrow/57000-books-with-metadata-and-blurbs"],"metadata":{"id":"e5jRS8iDsh68"}}]}